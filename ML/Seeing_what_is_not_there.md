# 基于上下文环境断定目标缺失 Seeing What Is Not There: Learning Context to Determine Where Objects Are Missing

## 1 摘要及简介

计算机视觉研究大多关注的是图像中有什么，本文提出并训练了一个独立以目标为中心的周边环境模型来完成相反的工作，即检测图像中缺少什么。
我们的周边环境模型可以检测到应该出现的物体，即使它没有出现。
模型基于卷积神经网络结构。通过一个特殊的训练策略，模型学着忽略目标物体本身儿专注于目标周边的环境。

从计算的角度看，在满足以下情况时，一个目标物体被认为在图像中缺失：

1. 目标检测器检测到不存在
1. 目标物体典型环境预测器表明其有很大概率存在

我们需要把所有这样的区域高效地检测出来。目标物体与环境的关系见下表：

| 目标检测器评分 | 环境预测器评分 | 图像区域标记 |
| :------: | :------: | :------: |
| 高 | 高 | 典型物体 |
| 低 | 高 | 物体缺失（该有却没有） |
| 高 | 低 | 不符合环境预期而出现的物体（不该有却有） |

提出这个课题的最初契机是街景路缘坡道（路口马路牙子便于残疾人上下的斜坡）检测问题。
标记城市路口路缘坡道的有无以便于残疾人规划其出行路线。

![](.Seeing_what_is_not_there_images\f1.png)

> 图注：当路口路缘坡道缺失（如图中左侧橙色区域，右侧绿色区域为正常）时，行动障碍人士将无法穿越这个路口。
我们提出了一种通过学习周边环境来检测物体缺失的模型，用来与目标检测的结果结合。

解决这个问题的关键在于训练一个仅关注周边环境的模型，就像一个目标检测器一样：通过扫描每张图片，获得一个概率映射热力图，
图中每一个像素代表这一点目标存在的概率，即使目标物体不存在。
这种周边环境和目标分离的好处在于，我们不需要再训练集中标记不正常（缺失、不符合环境预期而出现的物体）的目标物体，
独立周边环境模型可以由典型目标物体训练得到，而后用于寻找不正常目标物体。
这大大减轻了训练的工作量，因为正常物体的样本非常丰富，收集起来比不正常样本容易得多。

本文主要内容

1. 提出一种通过学习遮罩物体来获得以目标为中心的周边环境表示的训练方法。
1. 提出一种通过强制神经网络忽略物体本身，学习非显式遮罩的训练策略。
1. 应用于检测街景图中缺失路缘坡道的较理想结果；应用于检测不符合环境预期出现的人脸图像的初步结果。

## 2 相关工作

### 目标识别中的周边环境

认知科学认为周边环境影响人对物体的识别与判断。因此，此概念在计算机视觉中也逐渐被接受。

### 找到缺失目标

General Hough Transform：在目标跟踪的诊帧中检测缺失目标（可能是视频），其核心是通过周边物体的连续运动评估目标的位置。

### 带有遮罩图像的计算机视觉

Pathak et al.提出一个基于卷积神经网络的encoder用来自动填补画图（image inpainting）。与本文同用了遮罩，但他们训练生成式模型来填补遮罩，
而本文训练判别（discriminative）模型来推断遮罩后是什么。

### Accessibility Task

很多计算机算法利用网上大量的数据资源，例如谷歌街景等来帮助残障人士。 CrossingGuard是一套帮助视觉障碍行人导航通过十字路口的系统。
Tohme是半自动的结合众包和计算机视觉来收集城市中路缘坡道位置的系统，也使用谷歌街景图像。它使用DPM(Deformable Parts Models)作为路缘坡道检测器。
本文利用的数据集，是他们提供的一套包含1086个城市交叉路口街景路缘坡道的数据集。

## 3 从显式目标遮罩中学习周边环境

这部分介绍前文提到的周边环境学习算法的基本版本。
如果周边环境被定义为目标物体周围除了目标物体本身的内容，那么可以认为这个模型字面上说是在学习周边环境：
因为所有训练集中的目标物体都被遮罩掉了。

在一个图像二分类训练集上训练模型。

1. 正样本：目标物体在每张图中心，物体本身及其视觉延伸被黑框遮罩（预处理后值为零）。整个图片宽约是物体边界框的4倍。
1. 负样本：每张图片中心区域采用类似的遮罩层随机裁剪。被覆盖的区域与ground truth标记物体重合不大于Jaccard系数0.2。

若一幅图像中有多个目标物体，在每个正样本中仅遮罩一个。因为对于一个目标物体来说，其他目标物体可以视作有用的“周边环境”。例如，路缘坡道就经常成对出现。

训练模型Q，由带有池化和dropout的四层卷积层和两个全连接层构成。其结构见下表

| 层类型 | Shape | 参数个数 |
| :------: | :------: | :------: |
| Convolution2D | (3, 3, 32) | 896 | 
| Convolution2D | (3, 3, 32) | 9248 | 
| MaxPooling2D | (2, 2) | 0 |
| Dropout | - | 0 |
| Convolution2D | (3, 3, 64) | 18496 |
| Convolution2D | (3, 3, 64) | 36928 |
| MaxPooling2D | (2, 2) | 0 |
| Dropout | - | 0 |
| FullyConnected | (53*53*64, 256) | 46022912 |
| Dropout | - 0 |
| FullyConnected | (256, 2) | 514 | 

共46,088,994个参数

损失函数：使用交叉熵作为分类损失函数。

\begin{equation}
L_c = -Q_y(I_m) + log \sum_y e^{Q_y(I_m)}
\end{equation}

其中，$ y \in \{ 1,2 \} $是遮罩图片$ I_m $的groundtruth标签（1为正样本，2为负样本），
$ Q(I_m) $是2*1向量，表示Q的输出，$ Q_y(I_m) $表示第y个分量。

$$
Q(I_m) = \begin{bmatrix}
   Q_1(I_m) \\
   Q_2(I_m)
\end{bmatrix}
$$

全卷积层用于代替基础网络的最后三层。利用这个网络可以接受任意输入，无需显式遮罩。

| 层类型 | Shape | 参数个数 |
| :------: | :------: | :------: |
| Convolution2D | (53, 53, 256) | 46022912 |
| Dropout | - | 0 |
| Convolution2D | (1, 1, 2) | 514 |

测试时，利用滑动窗口生成概率热力图，每个像素代表此处可能含有目标物体的周边环境得分（context score）。
在每个位置应用固定大小（本文实现中使用224*224）中心遮罩的图像块输入基础网络。遮罩大小参照训练集根据经验决定。
    

## 4 学习隐式遮罩的全卷积模型 A Fully Convolutional Model that Learns Implicit Masks

上述基础网络存在几个问题：

1. 该网络倾向于artifacts。矩形遮罩图像会导致网络学习遮罩边界的低层次图像特征。
1. 网络要求每个输入有显式遮罩层。这在应用于图像的所有位置和尺度来生成热力图时非常不便。

将带有全连接层的卷积神经网络转换为全卷积网络有一套标准的程序。然而本文中的问题更加复杂。在训练过程中，
基础网络在输入图像的中心看到的全是0，所以由于没有梯度更新，对应位置的参数权重随机无法控制（arbitrary）。
如果应用一般方法转为全卷积网络用于非遮罩图像，从那些神经元里带来的输出可能会随机影响热力图中的结果。

新问题：能否训练一个能在心中自动忽视遮罩区域的只关注周边环境的全卷积的网络？

这个网络应该对于无论遮罩与否的图像都应该有有近似的输出。立足于这个概念，网络应该学习那些在遮罩和原始图像中都有的特征，即非遮罩区域的特征。

![](.Seeing_what_is_not_there_images\F2.png)

孪生训练全卷积周边环境神经网络 the Siamese trained Fully convolutional Context network (SFC)的训练方案图。
直觉上看，强制全卷积神经网络Q对于遮罩图像与原始图像输出相似的结果。并生成正确的分类标签。

> Siamese network 孪生神经网络，简单来说用于衡量两个输入的相似程度。孪生神经网络有两个输入（Input1 and Input2）,将两个输入feed进入两个神经网络（Network1 and Network2），这两个神经网络分别将输入映射到新的空间，形成输入在新的空间中的表示。通过Loss的计算，评价两个输入的相似度。

我们除了需要最小化基础网络中的$ L_c $(classification loss)还需要最小化$ L_d $(distance loss)，定义为：

\begin{equation}
L_d = \| Q_y(I_m) - Q(I) \|_{p}
\end{equation}

其中，$ Q(I_m) $是网络Q在输入为遮罩图像$ I_m $时的输出向量，Q(I)是输入为原始图像I时的输出。
$ \left\| \cdot \right\|_p $表示$ L_p $-范数。

以上结构是一个孪生网络，所以本模型是一个孪生训练全卷积周边环境神经网络 the Siamese trained Fully convolutional Context network (SFC)。
在$ L_d $中我们选用$ L_1 $范数。
训练时，无需再手工指定遮罩大小，因为这一信息已经包含在卷积参数矩阵中。

总损失函数定义为：

\begin{equation}
L = \lambda L_d + L_c 
\end{equation}

其中，本文中$ \lambda = 0.5 $

上述训练策略的好处：

1. SFC忽略物体遮罩区域，可直接应用于任意尺寸的新的无遮罩图像。且在生成稠密概率热力图时非常高效。
1. SFC不易偏向artifacts。而基础网络会学习沿着遮罩边缘的特征。由于这些特征在无遮罩图像中并不出现，SFC忽略它们。
1. 训练过程中，可以有效应用难分样本挖掘（hard negative mining）。在训练代（epoch）之间，可以将SFC应用于所有训练集图像来生成热力图，
找出高分false positive区域。由于全卷积效率很高，这个过程可被包含在训练中。

![](.Seeing_what_is_not_there_images\f3.png)

上：输入：街景全景图。

中：使用基本神经网络利用滑动窗口方法生成的热力图。由于较高的计算损失，空间分辨率低。

下：利用SFC生成的稠密热力图。对于1024*2048输入，基础网络需要5分钟生成中图，SFC只需不到4秒生成更高分辨率的图。

## 5 找到缺失物体区域步骤 Finding Missing Object Regions Pipeline

利用独立的周边环境网络（基本网络或SFC网络），找出缺失物体区域的步骤如下：

1. 通过周边环境网络Q生成环境热力图。热力图表示目标物体应该出现的位置。
1. 利用目标检测器生成目标检测结果。将目标检测结果转换为二值图像（0-1图像），转换方式为，
目标物体边界框以内均转为0，其它区域为1。
1. 将热力图与二值图执行按像素与操作，得到结果图，是目标物体应该出现但目标检测器未检测到物体的区域。
1. 根据结果图，取其高分区域（超过某一阈值），将原图中对应部分裁剪下来，得到物体缺失区域。

## 6 实验

### 6.1 模型本身的性质 Characteristics of the Trained Model

一、首先检测基本模型和SFC的灵敏度（sensitivity）。一个理想的模型应该对输入图像的中心区域有较小的响应（small response variations）。
测试时，每次通过增加一个像素小噪声改变测试集图像，记录改变前后网络每像素输出的$ L_2 $距离。

------
Figure 4

上图是基础网络和SFC在此测试中的对比，图中是20个不同图像测试结果之和。图中深色点代表高灵敏度点。
与基础网络相比，SFC在中间有一个明显的空框，表明在这一区域的变化对网络的输出几乎没有影响（SFC习得的非显示遮罩区域）。

二、基本模型和SFC在测试集上的$ L_d $（distance loss）。使用相同超参数和初始化设置训练的基础网络和SFC。

| 项目 | SFC | 基础网络 |
| :------: | :------: | :------: |
| $ L_d $（distance loss） | 0.041 | 2.27 |

较低的loss表示网络对于遮罩和非遮罩图片的输出更接近。

因此，SFC按预想中运行：

1. 习得非显示遮罩，因此对中心区域变化不敏感。
1. 分类任务中的有用特征主要由非遮罩区域获得。

### 6.2 寻找缺失路缘坡道 Finding Missing Curb Ramp Regions

#### 初始化设置

目标：在街景路缘坡道数据集中寻找缺失的路缘坡道。数据集包含1086个来自北美四个城市谷歌街景全景图，分别是华盛顿、巴尔的摩、洛杉矶和Saskatoon (Canada)。
每张全景图都是1024*2048像素，数据集中对于存在的路缘坡道提供bounding box标签，平均每张图有4个路缘坡道。
此外，在我们的训练和测试中，专家人工又标注了所有缺失的路缘坡道。

数据集被分为训练集和测试集，各占一半。所有图片转换为YUV色彩，所有通道归一化（normalized）到平均数为0且一个标准差。
使用数据集提供的路缘坡道检测器（目标检测器），叫做DPM（Deformable Part Model），参数均取默认值。

#### 训练

使用5000个又训练集生成的样本训练每一代（epoch），其中一半是正样本一半是负样本。下图是一些示例。

----
Figure 5
绿色矩形代表正样本，红色矩形代表负样本

在训练SFC时，每个样本有两个版本，原始图像和遮罩图像。
我们调整正样本图像的大小，让图像宽度为224像素且目标物体宽度接近55像素，每个负样本使用与正样本相同的遮罩大小来防止遮罩形状的过拟合。

使用Keras/Tensorflow框架。优化算法使用默认参数的Adadelta。由于它是自动调整学习率的方法，因此无需手工调整学习率。
训练集的20%用于early stopping验证。

#### 结果

基础网络采用滑动窗口方式生成热力图，生成时stride取10像素，遮罩宽度分别取50、70、100像素生成多个尺寸热力图。
SFC无需设置遮罩宽度，因此将输入图像大小缩放0.5、0.7、1.0倍。如此两个网络可见相似的图像金字塔（image pyramids）。
用DPM提供目标检测结果。每张全景图，生成结合检测和周边环境图的地图，裁剪大小为d*d的高分区域（超过某一阈值）。根据初步经验，阈值设为0.4。

人工验证结果，我们提供一个网页，展示所有找到区域的图片，由用户反馈是否为缺失路缘坡道区域。参见下图

----
Figure 6
每个缩略图是一个取得区域（retrieved region），下方是得分。用户点击缩略图来验证对应的图片。

基础网络和SFC的结果与三种baseline比较，random scores, spatial prior map（空间先验热力图）和Faster RCNN。

Random scores：给图像所有区域随机打分。

Spatial prior map：利用路缘坡道在街景图中先前的位置构建而成。我们用Spatial prior map代替context热力图作比较。
先统计所有路缘坡道在训练集中的所有空间分布，然后对该分布使用sigma=10的30*30像素高斯核进行平滑（smoothed）。如下图。

----
Figure 7
利用训练集中所有groundtruth位置生成的Spatial prior map。由于大部分全景图都是交叉路口，因此数据集中有较强的空间连续性。

Faster RCNN：利用缺失路缘坡道的标注，我们可以把这个问题当做目标识别，直接训练一个Faster RCNN检测器来识别。
这里的"正样本"是缺失的路缘坡道。由于Faster RCNN有能力识别周边环境，由于它是end-to-end approach。我们认为Faster RCNN是一个较强的baseline。

验证缺失路缘坡道需要专业知识。下图展示真实路缘坡道缺失区域对已访问区域的recall的对比。

----
Figure 8
基础及SFC模型表现大幅超越2个baseline（random scores和prior maps）。带有难分样本挖掘SFC具有最好的表现。
取得区域大小d=400像素，从543个测试图像中获得500个取得区域。

结果显示带有难分样本挖掘的SFC超越所有其他的方法，我们相信这是由于全卷积结构的高效性在训练和生成高分辨率周边环境热力图中的作用。

1. Spatial prior map表现尚可，证明数据集中的路缘坡道确实具有空间偏差（spatial bias）。与Spatial prior map不同，本文模型在没有这种偏差的数据集上一样表现良好。
1. 与SFC相比，Faster RCNN检测器的recall明细偏低。若有更多的训练数据，Faster RCNN可能表现更好。
1. SFC甚至不需要缺失路缘坡道的训练集，它从普通路缘坡道的周边环境中学得了特征和信息，而与缺失路缘坡道相比，普通路缘坡道数据更容易获得和标注。
1. SFC使用一个并不太先进的DPM检测器，77%的漏检是由于不准确的目标检测导致。

调整取得区域d的大小，400像素到100像素。随着区域缩小，缺失路缘坡道处于区域中心变得至关重要。

| Region Width | 400 | 200 | 100 |
| :------: | :------: | :------: | :------: |
| SFC | 21.8 | 24.1 | 21.6 |
| Spatial Prior | 10.2 | 6.8 | 3.2 |
| Random Scores | 3.8 | 1.6 | 0 |

上表显示，SFC在d减小后受影响不多，这是由于它找到的区域本身就处于中央（very well localized）。
Random scores和Spatial prior map在d缩小时表现变差。

#### 讨论

SFC大约能访问543张全景图测试集中的500个区域时，就找到27%的缺失路缘坡道。这是一个令人印象深刻的结果：

1. 整个过程非常高效，便于部署和扫描其他城市区域。例如，在纽约曼哈顿大约有2820个交叉路口，
系统仅仅几个小时就可以在一个160万人口的区域，找到缺失路缘坡道。
1. 研究表明路缘坡道状态呈现一致性（consistency）：如果一个路口缺失，那么附近路口也缺失的概率非常高。

### 6.3 无周边环境的人脸识别 Finding Out of Context Faces

在Wider face数据集中进行无周边环境人脸识别。使用上文寻找缺失物体类似的方法和最新的人脸检测器。

无周边环境的人脸定义为没有可见身体区域的人脸。下图展示了基本结果。

## 7 结论

提出并实现了一个独立的周边环境表示方法来找到图像中缺失物体。
模型基于卷积神经网络，提供了学习非显式遮罩的方法，以便于网络忽略目标物体本身儿专注于周边环境。
实验证明提出的方法在寻找缺失路缘坡道问题上有效且高效。

-----

### 附录：其他知识点

#### Precision 与 Recall
 
Precision（准确率）和Recall（召回率）是两个评价模型效果的指标。

例如一个从图像中识别飞机的问题，定义：

True positives : 飞机的图片正确识别成为飞机。 
True negatives: 不是飞机的图片正确识别为不是飞机的图片。 
False positives: 不是飞机的图片被错误识别成为飞机。 
False negatives: 是飞机的图片被错误识别成不是飞机的图片（没有识别出来）。

Precision其实就是在识别出来的图片中，True positives所占的比率： 
这里写图片描述 
其中的n代表的是(True positives + False positives)，也就是系统一共识别出来多少照片 。 
在这一例子中，True positives为3，False positives为1，所以Precision值是 3/（3+1）=0.75。 
意味着在识别出的结果中，飞机的图片占75%。

Recall 是被正确识别出来的飞机个数与测试集中所有飞机的个数的比值： 
这里写图片描述 
Recall的分母是(True positives + False negatives)，这两个值的和，可以理解为一共有多少张飞机的照片。 
在这一例子中，True positives为3，False negatives为2，那么Recall值是 3/（3+2）=0.6。 
意味着在所有的飞机图片中，60%的飞机被正确的识别成飞机.。

#### Jaccard Index IoU

> Jaccard Index （IoU）: 给定两个集合A,B的Jaccard系数定义为A与B交集的大小与并集大小的比值。

$$
J(A,B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A|+|B|-|A \cap B|}
$$

> Jaccard值越大说明相似度越高。当A和B相等时，Jaccard(A,B)=1； 

#### hard negative mining（难分样本挖掘）

把错误（尤其是顽固棘手的）送回训练集作为负样本，继续训练。

#### ADADELTA: An Adaptive Learning Rate Method

[https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)

Adagrad

Adagrad其实是对学习率进行了一个约束。即： 
nt=nt−1+g2tnt=nt−1+gt2n_t=n_{t-1}+g_t^2 
Δθt=−ηnt+ϵ−−−−−√∗gtΔθt=−ηnt+ϵ∗gt\Delta{\theta_t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t 
此处，对gtgtg_t从111到ttt进行一个递推形成一个约束项regularizer，−1∑tr=1(gr)2+ϵ√−1∑r=1t(gr)2+ϵ-\frac{1}{\sqrt{\sum_{r=1}^t(g_r)^2+\epsilon}} ，ϵϵ\epsilon用来保证分母非0

特点：


前期gtgtg_t较小的时候， regularizer较大，能够放大梯度
后期gtgtg_t较大的时候，regularizer较小，能够约束梯度
适合处理稀疏梯度


缺点：


由公式可以看出，仍依赖于人工设置一个全局学习率
ηη\eta设置过大的话，会使regularizer过于敏感，对梯度的调节太大
中后期，分母上梯度平方的累加将会越来越大，使gradient→0gradient→0gradient\to0，使得训练提前结束




Adadelta

Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 
Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。即： 
nt=ν∗nt−1+(1−ν)∗g2tnt=ν∗nt−1+(1−ν)∗gt2n_t=\nu*n_{t-1}+(1-\nu)*g_t^2 
Δθt=−ηnt+ϵ−−−−−√∗gtΔθt=−ηnt+ϵ∗gt\Delta{\theta_t} = -\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t

在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后： 
E|g2|t=ρ∗E|g2|t−1+(1−ρ)∗g2tE|g2|t=ρ∗E|g2|t−1+(1−ρ)∗gt2E|g^2|_t=\rho*E|g^2|_{t-1}+(1-\rho)*g_t^2 
Δxt=−∑t−1r=1Δxr−−−−−−−−√E|g2|t+ϵ−−−−−−−−√Δxt=−∑r=1t−1ΔxrE|g2|t+ϵ\Delta{x_t}=-\frac{\sqrt{\sum_{r=1}^{t-1}\Delta{x_r}}}{\sqrt{E|g^2|_t+\epsilon}} 
其中，EEE代表求期望。 
此时，可以看出Adadelta已经不用依赖于全局学习率了。

特点：


训练初中期，加速效果不错，很快
训练后期，反复在局部最小值附近抖动


[https://blog.csdn.net/u012759136/article/details/52302426](https://blog.csdn.net/u012759136/article/details/52302426) 


Keras 已收录
